{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch is an open-source deep learning framework developed by Meta AI, widely recognized for its flexibility and Pythonic design. Here's a concise breakdown of its key features and applications:\n",
        "\n",
        "**Core Features**  \n",
        "- **Dynamic Computation Graphs**: Enables real-time graph modifications during execution, contrasting with static graph frameworks like TensorFlow[2][4]. This \"eager execution\" approach simplifies debugging using standard Python tools[4][6].  \n",
        "- **GPU Acceleration**: Leverages NVIDIA's cuDNN/cuBLAS libraries for high-performance tensor operations, matching or exceeding TensorFlow's speed in many benchmarks[3][4].  \n",
        "- **TorchScript**: Allows seamless conversion between eager mode (for development) and optimized graph mode (for production deployment)[1][3].  \n",
        "- **Autograd System**: Implements reverse-mode auto-differentiation through a tape-based mechanism for efficient gradient calculations[3][5].  \n",
        "\n",
        "**Development Advantages**  \n",
        "- **Python Integration**: Feels native to Python developers, with API similarities to NumPy and support for common scientific libraries[5][6].  \n",
        "- **Research Focus**: Preferred in academia for rapid prototyping due to its modular design and dynamic nature[2][4].  \n",
        "- **Mobile Deployment**: Supports end-to-end workflows from Python to iOS/Android applications[1][3].  \n",
        "\n",
        "**Ecosystem & Community**  \n",
        "- **Growing Adoption**: Has surpassed TensorFlow in popularity among researchers, with a 28% decline in TensorFlow usage reported in 2025[5][6].  \n",
        "- **Rich Resources**: Extensive official documentation, tutorials, and third-party libraries (e.g., torchvision, Hugging Face Transformers)[5][6].  \n",
        "\n",
        "**Performance Considerations**  \n",
        "- **Benchmarks**: Shows competitive training speeds for models like ResNet-50 and MobileNet compared to TensorFlow[4].  \n",
        "- **Scalability**: Supports distributed training and multi-GPU configurations for large-scale applications[3][6].  \n",
        "\n",
        "While TensorFlow remains strong in production environments, PyTorch dominates research contexts due to its intuitive interface and flexibility[2][4][6]. For new projects, its balance of usability and performance makes it a compelling choice across both academic and industrial applications[1][5].\n",
        "\n",
        "Citations:\n",
        "[1] https://ai.meta.com/tools/pytorch/\n",
        "[2] https://builtin.com/data-science/pytorch-vs-tensorflow\n",
        "[3] https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/overview.html\n",
        "[4] https://viso.ai/deep-learning/pytorch-vs-tensorflow/\n",
        "[5] https://www.dataquest.io/blog/pytorch-for-deep-learning/\n",
        "[6] https://opencv.org/blog/pytorch-vs-tensorflow/\n",
        "[7] https://www.usdsi.org/data-science-insights/resources/pytorch-deep-learning-framework\n",
        "[8] https://www.reddit.com/r/MLQuestions/comments/112sege/pytorch_vs_tensorflow/\n",
        "[9] https://www.techtarget.com/searchenterpriseai/definition/PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**short summary comparing Keras (with TensorFlow backend) and PyTorch**:\n",
        "\n",
        "---\n",
        "\n",
        "| Feature              | **Keras (with TensorFlow)**                    | **PyTorch**                                |\n",
        "|----------------------|-----------------------------------------------|---------------------------------------------|\n",
        "| **Ease of Use**      | Very beginner-friendly, high-level API        | More code control, slightly steeper learning curve |\n",
        "| **Model Building**   | Functional/sequential API + subclassing       | Dynamic computation graph, very flexible     |\n",
        "| **Debugging**        | Harder (static graphs) — better with `tf.function` | Easier (eager execution by default)         |\n",
        "| **Community & Docs** | Large community, extensive documentation       | Strong research community, growing fast     |\n",
        "| **Deployment**       | Great support via TensorFlow Serving, TF Lite | TorchServe is improving but less mature     |\n",
        "| **Popularity**       | More used in production/enterprise (Google-backed) | More popular in research (Facebook-backed)  |\n",
        "\n",
        "---\n",
        "\n",
        "✅ **Use Keras/TensorFlow** for fast prototyping + production-ready workflows.  \n",
        "✅ **Use PyTorch** if you want low-level control and flexibility for research or custom training loops.\n",
        "\n",
        "Would you like a sample model in both to compare side-by-side?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbhIGhSZHPD-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbFO3iXEHQI3",
        "outputId": "3dee6f98-7779-4a83-c8cf-3cce86b4d19c"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing: normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)) #using mean and std values of 0.5 applies 0 to 1 normalization\n",
        "])\n",
        "\n",
        "# Loading the dataset\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: 60,000 images\n"
          ]
        }
      ],
      "source": [
        "print(f'Training data: {len(train_dataset):,} images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: 60000 samples\n",
            "Testing data: 10000 samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Printing the shape of the datasets\n",
        "print(f'Training data: {len(train_dataset)} samples')\n",
        "print(f'Testing data: {len(test_dataset)} samples')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.7412, -0.2471,  0.3725,  0.2235, -0.4980, -0.8902,\n",
            "          -0.5765,  0.0745,  0.6000,  0.5216, -0.2000, -1.0000, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4275,\n",
            "           0.4588,  0.3882,  0.4353,  0.3725,  0.4745,  0.8196,  1.0000,\n",
            "           0.7490,  0.7176,  0.5216,  0.4039,  0.4588,  0.6706,  0.1451,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.2784,\n",
            "           0.0980,  0.1765,  0.1922,  0.1765,  0.1451,  0.3725,  0.3725,\n",
            "           0.3569,  0.3412,  0.2235,  0.1922,  0.1608,  0.0118,  0.2235,\n",
            "           0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1765,  0.1137,\n",
            "           0.0980,  0.1922,  0.2549,  0.2235,  0.1451,  0.1137, -0.0039,\n",
            "           0.0588,  0.0431,  0.0980,  0.0980,  0.0745,  0.0431, -0.0196,\n",
            "           0.3255, -0.4118, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5765,  0.3098,\n",
            "           0.1451,  0.0118,  0.1137,  0.0745,  0.0745,  0.0275,  0.1608,\n",
            "           0.1608,  0.0431,  0.0275,  0.0275,  0.0275, -0.0196,  0.0980,\n",
            "           0.0980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1373,\n",
            "           0.4745,  0.0431,  0.1451,  0.1922,  0.0431, -0.0196, -0.0039,\n",
            "          -0.0667,  0.0118,  0.0431, -0.0667,  0.0980,  0.0275,  0.1765,\n",
            "          -0.8902, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "           0.7333,  0.2392,  0.0745,  0.0588, -0.0353, -0.1373, -0.1373,\n",
            "          -0.1059, -0.1529, -0.1216, -0.0824, -0.0039,  0.1137, -0.3961,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686, -1.0000,\n",
            "          -0.8039,  0.2392,  0.0745, -0.0196, -0.0667, -0.0667, -0.1373,\n",
            "          -0.0824, -0.0824, -0.1373, -0.0667, -0.0039,  0.1294, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.0353,  0.2235,  0.0118, -0.1216, -0.1373, -0.2000,\n",
            "          -0.1216, -0.2157, -0.0510, -0.0824,  0.0118, -0.1059, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.0196,  0.3255, -0.0039, -0.0667, -0.1686, -0.1529,\n",
            "          -0.1843, -0.2627, -0.0510, -0.1059,  0.0118, -0.2863, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
            "          -1.0000, -0.2314,  0.3412,  0.0118, -0.1216, -0.1843, -0.1059,\n",
            "          -0.1686, -0.2000, -0.1216, -0.1843,  0.0431, -0.4980, -1.0000,\n",
            "          -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
            "          -1.0000, -0.4824,  0.3569,  0.0588,  0.0118, -0.2314, -0.2157,\n",
            "          -0.0667, -0.2000, -0.1529, -0.2314,  0.0588, -0.5294, -1.0000,\n",
            "          -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
            "          -1.0000, -0.5608,  0.3412,  0.0588, -0.0039, -0.2157, -0.1529,\n",
            "          -0.0824, -0.3333, -0.1686, -0.1373,  0.0588, -0.4824, -1.0000,\n",
            "          -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.5922,  0.1765,  0.0118, -0.1373, -0.2157, -0.2863,\n",
            "          -0.2000, -0.2627, -0.3490, -0.1843, -0.0353, -0.4824, -1.0000,\n",
            "          -0.9686, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,\n",
            "          -1.0000, -0.4824,  0.3098,  0.0980,  0.1608,  0.1608, -0.0039,\n",
            "           0.0745,  0.1922,  0.1451,  0.1451,  0.1608, -0.2471, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.6471, -0.0353, -0.2627, -0.1843, -0.2471, -0.0667,\n",
            "          -0.0510, -0.1686, -0.2314, -0.1216, -0.3176, -0.1059, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -0.1686, -0.3020, -0.5451, -0.6078, -0.7098, -0.6078,\n",
            "          -0.4824, -0.5608, -0.6078, -0.4118, -0.4118,  0.0745, -0.8275,\n",
            "          -1.0000, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -1.0000,\n",
            "          -0.7725,  0.1608, -0.1059, -0.1686, -0.0196, -0.3020, -0.2157,\n",
            "           0.0431, -0.0824,  0.0275,  0.0275,  0.0275, -0.0196, -0.1216,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -0.2157, -0.1686, -0.1059, -0.2863,  0.0745, -0.5137, -0.2000,\n",
            "           0.0275, -0.3020,  0.0588, -0.1216,  0.0275, -0.1529,  0.0588,\n",
            "          -0.7098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "           0.1451, -0.2157, -0.1529, -0.2314,  0.1294, -0.5137, -0.1686,\n",
            "           0.0275, -0.3176,  0.0431, -0.1843,  0.2549, -0.0824, -0.0510,\n",
            "          -0.4667, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7412,\n",
            "          -0.0510, -0.1529, -0.2471, -0.2157,  0.0980, -0.4431, -0.1686,\n",
            "          -0.0039, -0.3333,  0.0980, -0.1843,  0.1765,  0.0980, -0.1059,\n",
            "          -0.3020, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5137,\n",
            "          -0.0667, -0.1216, -0.2000, -0.1373,  0.0745, -0.4118, -0.1686,\n",
            "           0.1294, -0.3647,  0.1294, -0.1529, -0.0824,  0.2078, -0.0824,\n",
            "          -0.1843, -0.8588, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4824,\n",
            "          -0.0510, -0.2000, -0.1216, -0.0824,  0.0275, -0.4275, -0.1843,\n",
            "           0.2235, -0.3961,  0.0745,  0.0588, -0.3490,  0.4039,  0.0118,\n",
            "          -0.0510, -0.7255, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n",
            "          -0.0039, -0.3647, -0.0196,  0.0431, -0.0667, -0.3804, -0.2157,\n",
            "           0.3255, -0.3490,  0.0118,  0.3725, -0.5294,  0.2784,  0.0588,\n",
            "           0.1451, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1686,\n",
            "           0.0118, -0.5137,  0.0980,  0.1294, -0.1529, -0.3333, -0.3490,\n",
            "           0.2392, -0.3333,  0.0118,  0.3725, -0.6235,  0.1451,  0.0431,\n",
            "           0.0588, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0824,\n",
            "          -0.0667, -0.3804,  0.0980,  0.1922, -0.2000, -0.3020, -0.1373,\n",
            "           0.0745, -0.2471,  0.1765,  0.5373, -0.3490,  0.1294,  0.0588,\n",
            "           0.0431, -0.3961, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.2078,\n",
            "          -0.0510, -0.3176,  0.0980,  0.2078, -0.1216, -0.2627, -0.5922,\n",
            "           0.1137, -0.2157, -0.3490,  0.1922, -0.3333,  0.2549,  0.0431,\n",
            "          -0.2157, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
            "         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9686,\n",
            "          -1.0000, -0.9843, -1.0000, -0.7255, -0.9686, -0.7412, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
            "          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]]) 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "image, label = train_dataset[3]  # Replace 'random_index' with an actual integer index\n",
        "print(image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Transform image back to a format suitable for display\n",
        "image = image.squeeze(0)  \n",
        "\n",
        "# Get clothing type based on label\n",
        "clothing_types = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "                  'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "clothing_type = clothing_types[label]\n",
        "\n",
        "# Plot the image with label\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f\"Clothing Type: {clothing_type}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M7B9NI0GHW6N"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module): #The CNN class inherits from nn.Module and defines the architecture of the neural network.\n",
        "    def __init__(self):\n",
        "        # we'll build 1 conv layer with max pooling\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 1 input channel, 32 output channels, 3x3 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n",
        "        self.fc1 = nn.Linear(13*13*32, 100)  # Flattened dimensions after pooling - done manually\n",
        "        self.fc2 = nn.Linear(100, 10)  # 10 classes for FashionMNIST\n",
        "#The forward method defines the forward pass of the network, specifying how input data flows through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 13*13*32)  # Flatten\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        return nn.Softmax(dim=1)(x)\n",
        "\n",
        "model = CNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The flattened dimensions in this CNN (13x13x32) are determined by calculating how the input image size changes after passing through the convolutional and pooling layers. Let me break down the calculation step by step:\n",
        "\n",
        "### Starting Dimensions\n",
        "For FashionMNIST dataset, the input images are 28×28 pixels with 1 channel.\n",
        "\n",
        "### After First Convolutional Layer (self.conv1)\n",
        "- Input: 28×28×1\n",
        "- Filter: 3×3 with stride 1, no padding\n",
        "- Output size calculation: (Input size - Filter size + 2×Padding)/Stride + 1\n",
        "- Width and height: (28 - 3 + 0)/1 + 1 = 26\n",
        "- Output: 26×26×32 (32 output channels defined in conv1)\n",
        "\n",
        "### After Pooling Layer (self.pool)\n",
        "- Input: 26×26×32\n",
        "- Pool size: 2×2\n",
        "- Output size calculation: Input size/Pool size\n",
        "- Width and height: 26/2 = 13\n",
        "- Output: 13×13×32\n",
        "\n",
        "### Flattening for Fully Connected Layer\n",
        "- The 3D tensor (13×13×32) is flattened into a 1D vector\n",
        "- Total elements: 13×13×32 = 5,408\n",
        "- This is why the input to fc1 is 13*13*32\n",
        "\n",
        "The dimensions could be calculated programmatically, but here they were likely worked out manually or through debugging. In more complex networks, developers often use print statements to check tensor dimensions at each step:\n",
        "\n",
        "```python\n",
        "def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    print(\"After conv1:\", x.shape)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = self.pool(x)\n",
        "    print(\"After pooling:\", x.shape)\n",
        "    # rest of the code\n",
        "```\n",
        "\n",
        "\n",
        "The dynamic approach makes the code more robust to input size changes or architecture modifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dynamic Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, 1)  # 1 input channel, 32 output channels, 3x3 kernel, stride 1\n",
        "    self.pool = nn.MaxPool2d(2, 2)       # 2x2 pooling with stride 2\n",
        "    \n",
        "    # Calculate the flattened dimension dynamically\n",
        "    with torch.no_grad():  # Disable gradient tracking for this calculation\n",
        "        # Create a sample input tensor with the expected input dimensions\n",
        "        # (batch_size=1, channels=1, height=28, width=28) for FashionMNIST\n",
        "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
        "        \n",
        "        # Apply convolution to the dummy input\n",
        "        x = self.conv1(dummy_input)  # Shape becomes [1, 32, 26, 26]\n",
        "        \n",
        "        # Apply pooling to the result\n",
        "        x = self.pool(x)  # Shape becomes [1, 32, 13, 13]\n",
        "        \n",
        "        # Calculate the total number of features after flattening\n",
        "        # Size(0) is batch dimension, so we multiply sizes 1, 2, and 3 (channels, height, width)\n",
        "        flattened_size = x.size(1) * x.size(2) * x.size(3)  # 32 * 13 * 13 = 5408\n",
        "        \n",
        "        print(f\"Dynamically calculated flattened size: {flattened_size}\")\n",
        "    \n",
        "    # Create fully connected layers using the dynamically calculated input size\n",
        "    self.fc1 = nn.Linear(flattened_size, 100)  # Connect flattened input to 100 neurons\n",
        "    self.fc2 = nn.Linear(100, 10)              # Connect to 10 output classes (FashionMNIST)\n",
        "\n",
        "def forward(self, x):\n",
        "    # Pass input through convolutional layer\n",
        "    x = self.conv1(x)            # Apply 32 convolutional filters\n",
        "    x = nn.ReLU()(x)             # Apply ReLU activation\n",
        "    x = self.pool(x)             # Apply max pooling\n",
        "    \n",
        "    # Flatten the 3D tensor (channels, height, width) to a 1D vector\n",
        "    # -1 in the first dimension means \"infer this dimension from the tensor's size\"\n",
        "    batch_size = x.size(0)       # Get the batch size\n",
        "    x = x.view(batch_size, -1)   # Reshape to [batch_size, flattened_size]\n",
        "    \n",
        "    # Pass through fully connected layers\n",
        "    x = self.fc1(x)              # First fully connected layer\n",
        "    x = nn.ReLU()(x)             # Apply ReLU activation\n",
        "    x = self.fc2(x)              # Output layer\n",
        "    return nn.Softmax(dim=1)(x)  # Apply softmax to get class probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYQXFP23HcoL",
        "outputId": "37e1b1d4-db21-43ad-d51b-53d6912b4033"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 1.7087, Accuracy: 75.50%\n",
            "Epoch 2/10, Loss: 1.5818, Accuracy: 88.03%\n",
            "Epoch 3/10, Loss: 1.5669, Accuracy: 89.53%\n",
            "Epoch 4/10, Loss: 1.5576, Accuracy: 90.39%\n",
            "Epoch 5/10, Loss: 1.5505, Accuracy: 91.12%\n",
            "Epoch 6/10, Loss: 1.5468, Accuracy: 91.47%\n",
            "Epoch 7/10, Loss: 1.5409, Accuracy: 92.04%\n",
            "Epoch 8/10, Loss: 1.5372, Accuracy: 92.42%\n",
            "Epoch 9/10, Loss: 1.5343, Accuracy: 92.69%\n",
            "Epoch 10/10, Loss: 1.5316, Accuracy: 92.99%\n"
          ]
        }
      ],
      "source": [
        "# Setting up the loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model for 10 epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate loss\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFubvmv-Hfmx",
        "outputId": "17fbbc41-c4ee-4f3c-d57e-55638e4b6384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model accuracy on test set: 90.08%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Model accuracy on test set: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8yRZqjlJ5HY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the model (optional)\n",
        "torch.save(model.state_dict(), 'fashion_mnist_cnn.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "model = FashionCNN()  # Create a new model instance\n",
        "model.load_state_dict(torch.load('fashion_mnist_cnn.pt'))\n",
        "model.eval()  # Set the model to evaluation mode (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flowers Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the device (GPU or CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the data transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load the Flowers dataset\n",
        "train_dataset = datasets.ImageFolder('flowers/train', data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder('flowers/val', data_transforms['val'])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class FlowerCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FlowerCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256*56*56, 128)\n",
        "        self.fc2 = nn.Linear(128, 5)  # 5 classes: daisy, dandelion, roses, sunflowers, tulips\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.max_pool(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.max_pool(x)\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.max_pool(x)\n",
        "        x = x.view(-1, 256*56*56)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = FlowerCNN()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}')\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    print(f'Epoch {epoch+1}, Val Loss: {val_loss / len(val_loader)}, Val Acc: {accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
