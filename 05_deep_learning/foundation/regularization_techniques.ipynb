{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is used to reduce overfitting, improve generalization, and enhance the model performance on unseen data. Below are the most common techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization (Lasso)\n",
    "- It penalizes the model by adding a term to the loss function that discourages large weights (penalty)\n",
    "- Adds the absolute value of the weights to the loss function\n",
    "- Tends to drive the weights to zero\n",
    "- Applied an individual layer (recommendation: hidden layer with highest complexity)\n",
    "\n",
    "![lasso](https://www.researchgate.net/publication/352564627/figure/fig1/AS:1037128163135494@1624282031207/Neural-network-representation-of-the-lasso-problem-Equation-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = models.Sequential([\n",
    "                          layers.Input(shape=(28,28)),\n",
    "                          layers.Flatten(),\n",
    "                          layers.Dense(140, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "                          layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization (Ridge)\n",
    "- It penalizes the model by adding a term to the loss function that discourages large weights (penalty)\n",
    "- Adds the square of the weights to the loss function\n",
    "- Tends to shrink the weights, but doesn't drive them to absolute zero\n",
    "- Applied an individual layer (recommendation: hidden layer with highest complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "nn_model = models.Sequential([\n",
    "                        layers.Input(shape=(28,28)),\n",
    "                        layers.Flatten(),\n",
    "                        layers.Dense(140, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "                        layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Combined (ElasticNet)\n",
    "- Same attributes from both L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "nn_model = keras.models.Sequential([\n",
    "                        layers.Flatten(input_shape=(28, 28)),\n",
    "                        layers.Dense(140, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)), # notice the very small values\n",
    "                        layers.Dense(10)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Randomly sets a fraction of the neurons to zero during training \n",
    "- Introduces noise and prevents co-adaptation of neurons (reduces interdependence)\n",
    "- It's added as layer object, but technically applied to the previous layer\n",
    "- Applied to layers individually (recommendation: hidden layer with highest complexity)\n",
    " \n",
    "![dr](https://dataheroes.ai/wp-content/uploads/2023/04/post-2378-03.png)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_model = keras.models.Sequential([\n",
    "                        layers.Flatten(input_shape=(28, 28)),\n",
    "                        layers.Dense(140, activation='relu'),\n",
    "                        layers.Dropout(0.1), #Fraction of the input units to randomly drop (10% dropped)\n",
    "                        layers.Dense(10)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Normalization** It normalizes the output of the activations of each layer to have zero mean and 1 std (unit variance)\n",
    "- **Scaling and Shifting** using 2 parameters (gamma and beta) it shifts the output\n",
    "- Applied to layers individually (recommendation: hidden layer with highest complexity)\n",
    "\n",
    "![bn](https://miro.medium.com/v2/resize:fit:709/1*Y0EtAQpR2iBsv97YrwRywg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn_model = keras.models.Sequential([\n",
    "                        layers.Flatten(input_shape=(28, 28)),\n",
    "                        layers.Dense(140, activation='relu'),\n",
    "                        layers.BatchNormalization(),\n",
    "                        layers.Dense(140, activation='relu'),\n",
    "                        layers.Dense(10)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It monitors the validation loss and stops training when it [validation loss] stops improving \n",
    "- Basically, the learning stops when the performance of the model on a validation subset starts degrading\n",
    "- Unlike other techniques, it is applied in `fit()` and not in the model architecture \n",
    "- You set 2 key parameters:\n",
    "    - `monitor`:The measure to be monitored  (val_loss)\n",
    "    - `patience`: Number of epochs with no improvement after which training will be stopped. \n",
    "\n",
    "![ES](https://miro.medium.com/v2/resize:fit:708/1*LSjaVNMa-ku85I35of-mAw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural net structure\n",
    "nn_model = keras.models.Sequential([\n",
    "                        layers.Flatten(input_shape=(28, 28)),\n",
    "                        layers.Dense(140, activation='relu'), #activation=tf.nn.relu\n",
    "                        layers.BatchNormalization(),\n",
    "                        layers.Dense(10)\n",
    "                      ])\n",
    "#compiling \n",
    "nn_model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "# apply early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2) # applied after 2 Epochs\n",
    "\n",
    "model_hist = nn_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus** you can add additional features in the callbacks attributes such as:\n",
    "- EarlyStopping\n",
    "- Model CheckPoint to record the learning snapshot in every Epoch\n",
    "- Connect the model performance to TensorBoard for monitoring and visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=2),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "model.fit(dataset, epochs=10, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing The Right Regularization Technique\n",
    "- It depends on the specific problem and dataset.\n",
    "- Therefore, it's a trial and error process to find the optimal method or combination of methods (using hyperparameter tuning)\n",
    "- Recommendations:\n",
    "    - **Fine Tuning** Start with a low level of regularization and then start increasing until you get the desired outcome\n",
    "    - **Model Complexity** complex models may need stronger regularization e.g. multiple layers and large number of neurons\n",
    "    - **Dataset Size** larger datasets don't need high level of regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
