{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction\n",
    "- Scalers, Vectors, and Matrices\n",
    "- Vector Operations\n",
    "    - Arithmetics \n",
    "    - Norm\n",
    "- Matrix Operations\n",
    "    - Arithmetics \n",
    "    - Rank of a matrix\n",
    "    - Inverse\n",
    "    - Determinant\n",
    "    - Identity\n",
    "- Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Linear Algebra?\n",
    "- It's a branch of mathematics and helps with having a deep understanding of data science and machine learning concepts.\n",
    "- In Lin Alg data is represented by scalers, vectors, and matrices with linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalers and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Scaler** represents a magnitude or measurement. e.g. length, area, volume, speed, etc..\n",
    "- **Vector** it's a physical quantity that has direction and magnitude. e.g.\n",
    "    - Wind velocity is a vector: speed and direction of the wind. 15 miles/hour northeast\n",
    "    - If you have an image array, you can use a vector for each color. Magnitudes represent the intensity of the color.\n",
    "    - Weight in physics has direction (gravity) and magnitude\n",
    "- If we have `v = [4,5,6]` the norm of the vector is the magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_3.png)\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying 2 vectors (Dot Product vs Cross Product)\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dot Product:\n",
    "    - The result is always a scaler. \n",
    "    - Also called scaler product.\n",
    "    - Applications:\n",
    "        - Finding the angle between vectors\n",
    "        - Project one vector onto another\n",
    "- Cross Product\n",
    "    - The result is always a vector\n",
    "    - The resulting vector is perpendicular to both input vectors\n",
    "    - It works in 3D space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> two vectors must be the same size for the operations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [4,5,6]\n",
    "b = [8,9,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-44,  40,  -4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cross(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 14,  8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a,b) #lin alg concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8, 9, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concat - stitching the data \n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4, -4,  4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.subtract(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norm of A Vector\n",
    "- It calculates the vector's magnitude (it's a measurement of length)\n",
    "- It's always a positive value\n",
    "- A numerical example of the norm of a vector:\n",
    "\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.774964387392123"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [4,5,6]\n",
    "#use the linalg submodule \n",
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A matrix is a rectangular array of numbers or expressions, arranged in columns and rows.\n",
    "- The definition differs about the number of dimensions. In DS, they call 2d array Matrix and 2+ d arrays Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sh](https://res.cloudinary.com/practicaldev/image/fetch/s--8pw60d5S--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_800/https://raw.githubusercontent.com/adhiraiyan/DeepLearningWithTF2.0/master/notebooks/figures/fig0201a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6, 7],\n",
       "       [7, 4, 8]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.array([[5,6,7],[7,4,8]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![m](https://i.ytimg.com/vi/d6lIaqQI0UE/sddefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![matrizmulti1.PNG](https://s3.us-east-1.amazonaws.com/static2.simplilearn.com/lms/testpaper_images/ADSP/Advanced_Statistics/LinearRegression/matrizmulti1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[5,6,7],[7,4,8],[4,3,2]])\n",
    "b = np.array([[7,2,1],[6,6,3],[7,5,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "square matrix: number of rows =number of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication (Square Matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120,  81,  37],\n",
       "       [129,  78,  35],\n",
       "       [ 60,  36,  17]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120,  81,  37],\n",
       "       [129,  78,  35],\n",
       "       [ 60,  36,  17]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b # syntax 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-Wise Matrix Multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, 12,  7],\n",
       "       [42, 24, 24],\n",
       "       [28, 15,  4]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank of A Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To find the rank of a matrix, first convert it into the row echelon form.\n",
    "\n",
    "For a matrix to be in its echelon form, it must follow these three rules:\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Example__\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_12.png)\n",
    "\n",
    "The output, after using elementary transformations, is shown below:\n",
    "\n",
    "**R2 → R2 – 2R1**\n",
    "\n",
    "**R3 → R3 – 3R1**\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_13.png)\n",
    "\n",
    "**R3 → R3 – 2R2**\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_14.png)\n",
    "\n",
    "The above matrix is in row-echelon form.\n",
    "\n",
    "Since the number of non-zero rows is 2,\n",
    "\n",
    "\n",
    "\n",
    "**The rank of the matrix is 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 1, 4],\n",
       "       [3, 0, 5]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3],\n",
    "              [2,1,4],\n",
    "              [3,0,5]\n",
    "              ])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of matrix a: 2\n"
     ]
    }
   ],
   "source": [
    "print('Rank of matrix a:', np.linalg.matrix_rank(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant and Trace of a Matrix\n",
    "\n",
    "The determinant of a matrix is a scalar quantity that is a function of the elements of the matrix.\n",
    "- Determinants are defined only for square matrices.\n",
    "- These are useful in determining the solution of a system of linear equations.\n",
    "\n",
    "                               Let X = [aij] be an nxn matrix, where n ≥2\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_15.png)\n",
    "\n",
    "\n",
    "**Note:** The determinant of a non-square matrix is not defined. The determinant of a matrix X is denoted by det X or |X|.\n",
    "\n",
    "\n",
    "**Consider the matrices 2X2 and 3X3:**\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_16.png)\n",
    "\n",
    "   \n",
    "Substitute the expressions for the determinant of a $ 2\\times 2 $ matrix in the above equation. So, the output will be shown as below:\n",
    "\n",
    "![det2.PNG](https://s3.us-east-1.amazonaws.com/static2.simplilearn.com/lms/testpaper_images/ADSP/Advanced_Statistics/LinearRegression/det2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of a: 6.66133814775094e-16\n"
     ]
    }
   ],
   "source": [
    "print(\"Determinant of a:\", np.linalg.det(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a square matrix is the sum of its diagonal entries.</br>\n",
    "![tr](https://media.geeksforgeeks.org/wp-content/uploads/20221119211255/tm2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace of a: 7\n"
     ]
    }
   ],
   "source": [
    "# Trace of matrix A\n",
    "print(\"Trace of a:\", np.trace(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Matrix or Operator\n",
    "\n",
    "An identity matrix (I) is a square matrix that, when multiplied with a matrix X, gives the same result as X.\n",
    "\n",
    "![det3.PNG](https://s3.us-east-1.amazonaws.com/static2.simplilearn.com/lms/testpaper_images/ADSP/Advanced_Statistics/LinearRegression/det3.PNG)\n",
    "\n",
    "**Hint**: This is equivalent to the number 1 in the number system.\n",
    "\n",
    "\n",
    "The diagonal elements of I are all 1, and all its non-diagonal elements are 0.\n",
    "\n",
    "#### Example:\n",
    "![det4.PNG](https://s3.us-east-1.amazonaws.com/static2.simplilearn.com/lms/testpaper_images/ADSP/Advanced_Statistics/LinearRegression/det4.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvalues are a special set of scalar values associated with the linear equations in matrix operations. Imagine you have a big box. You're asked to stretch or shrink everything inside the box, but only in certain directions. Eigenvalues tell you how much everything stretches or shrinks along those special directions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors represent directions in which the linear transformation has a stretching or compressing effect. Imagine you have arrows inside the box. These arrows represent different directions. Eigenvectors are the special arrows that don't change their direction when you apply force. They might get longer or shorter, but they still point in the same direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are used in the following areas:\n",
    "\n",
    "- Linear transformations: Eigenvalues and eigenvectors understand and analyze the behavior of linear transformations. They provide insights into how the transformation affects different directions in the vector space.\n",
    "\n",
    "- Differential equations: Eigenvalues and eigenvectors solve systems of ordinary and partial differential equations. They help find solutions that have exponential growth or decay behavior.\n",
    "\n",
    "- Structural analysis: In structural engineering, eigenvalues and eigenvectors analyze the stability and modes of vibration of structures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be an $ n\\times n $ matrix. A scalar $ \\lambda $  is called an Eigenvalue of X if there is a nonzero vector A such that AX =  $ \\lambda $A.  In this context, the vector A is called an eigenvector of X corresponding to $ \\lambda $.\n",
    "\n",
    "![link text](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/ADSP_Images/Lesson_06_Maths_and_Stats/Linear_Algebra/Image_19.png)\n",
    "\n",
    "Suppose X is an $ n\\times n $ matrix. When you multiply X with a new vector A, it does two things to the vector A:\n",
    "\n",
    "1.\tIt scales the vector.\n",
    "\n",
    "2.\tIt rotates the vector.\n",
    "\n",
    "When X acts on a certain set of vectors, it results in scaling the vector and not changing the direction of the vector.\n",
    "- These specific vectors, which you do not rotate but may stretch or compress, are called eigenvectors.\n",
    "- The amount by which these vectors stretch or compress is called the corresponding eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Applications\n",
    "\n",
    "Linear algebra concepts are foundational to many machine learning algorithms. Here are some key concepts and their applications:\n",
    "\n",
    "1. Vectors and Matrices:\n",
    "    - Used to represent data, parameters, and operations in ML models\n",
    "    - Enable efficient computation and storage of large datasets\n",
    "\n",
    "2. Matrix Operations:\n",
    "    - Matrix Multiplication:\n",
    "        - Neural network layers processing and transformations: RNN and CNN\n",
    "        - Recommendation Systems\n",
    "        - Dimensionality Reduction\n",
    "    - Element by element multiplication is used in:\n",
    "        - The attention mechanism, which is one of the key principles of transformers in GPT architecture.\n",
    "        - Feature scaling and normalization \n",
    "        - Activation Functions for neural networks\n",
    "    - Transpose: \n",
    "        - backpropagation\n",
    "        - data reshaping\n",
    "    - Inverse: \n",
    "        - Linear regression\n",
    "        - Principal Component Analysis (PCA)\n",
    "\n",
    "3. Eigenvalues and Eigenvectors:\n",
    "    - Principal Component Analysis (PCA): Dimensionality reduction\n",
    "    - Spectral clustering: Unsupervised learning technique\n",
    "\n",
    "4. Vector Spaces and Subspaces:\n",
    "    - Feature spaces: Represent data in high-dimensional spaces\n",
    "    - Kernel methods: Transform data into higher-dimensional spaces\n",
    "\n",
    "5. Linear Transformations:\n",
    "    - Neural network layers: Apply linear transformations to inputs\n",
    "    - Data preprocessing: Normalize or standardize input features\n",
    "    \n",
    "6. Singular Value Decomposition (SVD):\n",
    "    - Matrix factorization: Used in recommender systems\n",
    "    - Dimensionality reduction: Alternative to PCA\n",
    "\n",
    "7. Least Squares:\n",
    "    - Linear regression: Fitting models to data\n",
    "    - Optimization: Minimizing error in various ML algorithms\n",
    "\n",
    "8. Gradient Descent:\n",
    "    - Optimization: Minimizing loss functions in various ML algorithms\n",
    "\n",
    "9. Norm of a Matrix:\n",
    "    - Loss functions: Measuring model performance\n",
    "    - Regularization: Preventing overfitting\n",
    "\n",
    "\n",
    "- Matrix factorization (dot prod, rank, etc..):\n",
    "    - Recommendation Systems\n",
    "- Norm of a matrix:\n",
    "    - Optimization: Loss function and regularization\n",
    "    - Feature selection\n",
    "    - Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
