{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some cool and useful functions you can create for data analysis in Python:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically detect and handle missing data: \n",
    "Create a function that scans a DataFrame for missing values, and fills them in with appropriate values (e.g. mean, median, mode) or removes the rows/columns with too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing(df):\n",
    "    \"\"\"Fill missing values with mean, median or mode and/or drop rows/cols\"\"\" \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype == 'object': \n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].median()) \n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated data visualization: \n",
    "Write a function that takes in a DataFrame and automatically generates visualizations based on data types and distributions. For example, call histogram for numerical columns, boxplot for categorical columns, correlation matrix for all columns etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def auto_visualize(df):\n",
    "    \"\"\"Generate visualizations based on data type\"\"\"\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col].value_counts().plot(kind='bar')\n",
    "        elif df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "            df[col].hist()\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering helper:\n",
    "Make a function that takes in a DataFrame and allows you to easily create new features by applying operations on existing columns. For example: divide numeric columns, concatenate text columns, extract date/time information etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Perform feature engineering\"\"\"\n",
    "    \n",
    "    df['age_cat'] = df['age'].apply(lambda x: 'young' if x < 30 else 'old') \n",
    "    df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizable DataFrame summary: \n",
    "Create a function that prints key statistics on a DataFrame, with options to customize what's included - things like count, mean, std dev, min, max for numeric cols, value counts for categorical etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Automated model training: Make a function that automatically trains a bunch of models on a dataset to find the best performing one. Have it split data, instantiate/fit models like RandomForest, SVM, NeuralNet etc. and output metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "\n",
    "def auto_train_models(df, target):\n",
    "    \"\"\"\n",
    "    Automatically train and evaluate SVM, Random Forest and Neural Net models.\n",
    "    Output accuracy scores for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data\n",
    "    X = df.drop(target, axis=1) \n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    svm = SVC().fit(X_train, y_train)\n",
    "    rf = RandomForestClassifier().fit(X_train, y_train) \n",
    "    nn = MLPClassifier().fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    print(\"SVM Accuracy:\", accuracy_score(y_test, svm.predict(X_test)))\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_test, rf.predict(X_test))) \n",
    "    print(\"Neural Network Accuracy:\", accuracy_score(y_test, nn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use\n",
    "auto_train_models(df, 'target_col_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model comparison function: Create a function that takes in multiple trained models and a test set, runs predictions and outputs a table/chart to easily compare performance across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compare performance of multiple ML models, print scores \n",
    "    and plot confusion matrices.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = pd.DataFrame(columns=['model', 'accuracy', 'precision', 'recall', 'f1'])\n",
    "    \n",
    "    for model in models:\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate scores\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred) \n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Append scores\n",
    "        results = results.append({'model': model.__class__.__name__, \n",
    "                                  'accuracy': acc, \n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1}, ignore_index=True)\n",
    "                                  \n",
    "    # Print scores                                  \n",
    "    print(results)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    for i, model in enumerate(models):\n",
    "        \n",
    "       disp = plot_confusion_matrix(model, X_test, y_test)  \n",
    "       disp.figure_.suptitle(model.__class__.__name__)\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The key is to automate repetitive tasks and build reusable functions that make your analysis & modeling workflows more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple pipeline using Scikit-learn\n",
    "This example demonstrates a basic pipeline using Scikit-learn to perform data preprocessing, feature extraction, and classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here are some examples of pipeline automation for machine learning in Python:\n",
    "\n",
    "**Example 1: Building a simple pipeline using Scikit-learn**\n",
    "\n",
    "This example demonstrates a basic pipeline using Scikit-learn to perform data preprocessing, feature extraction, and classification.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "**Example 2: Automating hyperparameter tuning using GridSearchCV**\n",
    "\n",
    "This example illustrates how to automate hyperparameter tuning using GridSearchCV within a pipeline.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'classifier__alpha': [0.1, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the optimal hyperparameters\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best score:', grid_search.best_score_)\n",
    "```\n",
    "\n",
    "**Example 3: Implementing a custom pipeline component**\n",
    "\n",
    "This example demonstrates how to create a custom pipeline component for data preprocessing.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "class DataCleaner:\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Perform data cleaning operations\n",
    "        # ...\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Clean and transform the data\n",
    "        # ...\n",
    "\n",
    "        return X_cleaned\n",
    "\n",
    "# Create a pipeline object\n",
    "pipeline = Pipeline([\n",
    "    ('data_cleaner', DataCleaner()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline on the data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "These examples showcase the flexibility and power of pipeline automation for machine learning in Python. By combining various components and automating hyperparameter tuning, you can streamline the development and evaluation of machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
